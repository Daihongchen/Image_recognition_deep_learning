{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "                rescale=1./255)\n",
    "\n",
    "train_path = os.path.join(os.pardir, os.pardir, 'data/train')\n",
    "val_path = os.path.join(os.pardir, os.pardir, 'data/val')\n",
    "test_path = os.path.join(os.pardir, os.pardir, 'data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test known model to ensure functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (image_size, image_size, 3)\n",
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  keras.layers.GlobalAveragePooling2D(),\n",
    "  keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = train_generator.n // batch_size\n",
    "validation_steps = test_generator.n // batch_size\n",
    "\n",
    "history = model1.fit_generator(train_generator,\n",
    "                              steps_per_epoch = steps_per_epoch,\n",
    "                              epochs=epochs,\n",
    "                              workers=4,\n",
    "                              validation_data=test_generator,\n",
    "                              validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Made Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some convolutional layers, along with basic dropout for regularization \n",
    "\n",
    "model2 = tf.keras.Sequential()\n",
    "\n",
    "model2.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "model2.add(layers.Dropout(.5))\n",
    "model2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "model2.add(layers.Dropout(.3))\n",
    "model2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(64, activation='relu'))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.fit_generator(train_generator,\n",
    "                     steps_per_epoch = steps_per_epoch,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator,\n",
    "                     validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing images to 160x160\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 160\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_generator160 = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "test_generator160 = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some convolutional layers, along with basic dropout for regularization, \n",
    "\n",
    "model3 = tf.keras.Sequential()\n",
    "\n",
    "model3.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "model3.add(layers.Dropout(.5))\n",
    "model3.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "model3.add(layers.Dropout(.3))\n",
    "model3.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model3.add(layers.Flatten())\n",
    "model3.add(layers.Dense(64, activation='relu'))\n",
    "model3.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch160 = train_generator160.n // batch_size\n",
    "validation_steps160 = test_generator160.n // batch_size\n",
    "\n",
    "model3.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model3.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysis: variant accuracy indicated dropout may be too high early in network, unclear if issue is overfit or dropout coincidence\n",
    "\n",
    "solve: remove dropout entirely, then add backwards (from output layer to input layer), until stable and accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 160\n",
    "batch_size = 32\n",
    "\n",
    "train_generator160 = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "test_generator160 = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some convolutional layers, along with basic dropout for regularization, \n",
    "\n",
    "model4 = tf.keras.Sequential()\n",
    "\n",
    "model4.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model4.add(layers.MaxPooling2D((2, 2)))\n",
    "model4.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model4.add(layers.MaxPooling2D((2, 2)))\n",
    "model4.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model4.add(layers.Flatten())\n",
    "model4.add(layers.Dense(64, activation='relu'))\n",
    "model4.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch160 = train_generator160.n // batch_size\n",
    "validation_steps160 = test_generator160.n // batch_size\n",
    "\n",
    "model4.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model4.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  increasing batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 160\n",
    "batch_size64 = 64\n",
    "\n",
    "train_generator160 = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size64,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "test_generator160 = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size64,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = tf.keras.Sequential()\n",
    "\n",
    "model5.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model5.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model5.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model5.add(layers.Flatten())\n",
    "model5.add(layers.Dense(64, activation='relu'))\n",
    "model5.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch160 = train_generator160.n // batch_size64\n",
    "validation_steps160 = test_generator160.n // batch_size64\n",
    "\n",
    "model5.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model5.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysis:  batch size 64 performed slightly better, but lack of regularization hurt the overall performance\n",
    "\n",
    "solve: add batch normalization to each layer, and view results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regularization: batch normalization w/ batch size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 160\n",
    "batch_size64 = 64\n",
    "\n",
    "train_generator160 = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size64,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "test_generator160 = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size64,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = tf.keras.Sequential()\n",
    "\n",
    "model6.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model6.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model6.add(layers.BatchNormalization())\n",
    "model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model6.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model6.add(layers.BatchNormalization())\n",
    "model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model6.add(layers.Flatten())\n",
    "model6.add(layers.Dense(64, activation='relu'))\n",
    "model6.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch160 = train_generator160.n // batch_size64\n",
    "validation_steps160 = test_generator160.n // batch_size64\n",
    "\n",
    "model6.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model6.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regularization: batch normalization w/ batch size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = 160\n",
    "batch_size = 32\n",
    "\n",
    "train_generator160 = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "test_generator160 = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0414 14:26:52.197391 4722408896 deprecation.py:506] From /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model7 = tf.keras.Sequential()\n",
    "\n",
    "model7.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model7.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model7.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model7.add(layers.BatchNormalization())\n",
    "model7.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model7.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model7.add(layers.BatchNormalization())\n",
    "model7.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model7.add(layers.Flatten())\n",
    "model7.add(layers.Dense(64, activation='relu'))\n",
    "model7.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0414 14:26:53.204540 4722408896 deprecation.py:323] From /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "163/163 [==============================] - 266s 2s/step - loss: 0.2270 - acc: 0.9304 - val_loss: 0.9777 - val_acc: 0.6217\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 229s 1s/step - loss: 0.0756 - acc: 0.9751 - val_loss: 1.2767 - val_acc: 0.6234\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 226s 1s/step - loss: 0.0453 - acc: 0.9837 - val_loss: 1.1286 - val_acc: 0.6546\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 233s 1s/step - loss: 0.0283 - acc: 0.9898 - val_loss: 1.1076 - val_acc: 0.7664\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 207s 1s/step - loss: 0.0156 - acc: 0.9937 - val_loss: 3.9594 - val_acc: 0.6826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13a9c75f8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch160 = train_generator160.n // batch_size\n",
    "validation_steps160 = test_generator160.n // batch_size\n",
    "\n",
    "model7.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model7.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adding dropout layers / increasing total layers of convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = 160\n",
    "batch_size = 32\n",
    "\n",
    "train_generator160 = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "test_generator160 = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = tf.keras.Sequential()\n",
    "\n",
    "model8.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model8.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model8.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model8.add(layers.BatchNormalization())\n",
    "model8.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model8.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model8.add(layers.BatchNormalization())\n",
    "model8.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model8.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "model8.add(layers.BatchNormalization())\n",
    "model8.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model8.add(layers.Flatten())\n",
    "model8.add(layers.Dense(256, activation='relu'))\n",
    "model8.add(layers.Dropout(.3))\n",
    "model8.add(layers.Dense(128, activation='relu'))\n",
    "model8.add(layers.Dropout(.2))\n",
    "model8.add(layers.Dense(64, activation='relu'))\n",
    "model8.add(layers.Dropout(.1))\n",
    "\n",
    "model8.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "163/163 [==============================] - 281s 2s/step - loss: 0.2128 - acc: 0.9231 - val_loss: 2.2559 - val_acc: 0.6266\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 273s 2s/step - loss: 0.1299 - acc: 0.9555 - val_loss: 4.1423 - val_acc: 0.6266\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 281s 2s/step - loss: 0.0967 - acc: 0.9703 - val_loss: 2.2651 - val_acc: 0.6414\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 317s 2s/step - loss: 0.0745 - acc: 0.9757 - val_loss: 0.3036 - val_acc: 0.8914\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 270s 2s/step - loss: 0.0586 - acc: 0.9835 - val_loss: 5.3896 - val_acc: 0.6447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13aff86d8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch160 = train_generator160.n // batch_size\n",
    "validation_steps160 = test_generator160.n // batch_size\n",
    "\n",
    "model8.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model8.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analysis\n",
    "\n",
    "every model beyond model 3 has only decreased validation accuracy\n",
    "\n",
    "removing batch normalization from model 8 to investigate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = 160\n",
    "batch_size = 32\n",
    "\n",
    "train_generator160 = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "test_generator160 = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model9 = tf.keras.Sequential()\n",
    "\n",
    "model9.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model9.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model9.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model9.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model9.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model9.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model9.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "model9.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model9.add(layers.Flatten())\n",
    "model9.add(layers.Dense(256, activation='relu'))\n",
    "model9.add(layers.Dropout(.3))\n",
    "model9.add(layers.Dense(128, activation='relu'))\n",
    "model9.add(layers.Dropout(.2))\n",
    "model9.add(layers.Dense(64, activation='relu'))\n",
    "model9.add(layers.Dropout(.1))\n",
    "\n",
    "model9.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "163/163 [==============================] - 201s 1s/step - loss: 0.4682 - acc: 0.7960 - val_loss: 0.3398 - val_acc: 0.8651\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 199s 1s/step - loss: 0.1990 - acc: 0.9224 - val_loss: 0.6866 - val_acc: 0.7763\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 223s 1s/step - loss: 0.1469 - acc: 0.9461 - val_loss: 0.6164 - val_acc: 0.7763\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 207s 1s/step - loss: 0.1094 - acc: 0.9601 - val_loss: 0.6047 - val_acc: 0.8224\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 208s 1s/step - loss: 0.0979 - acc: 0.9628 - val_loss: 0.8417 - val_acc: 0.7862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13c743748>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch160 = train_generator160.n // batch_size\n",
    "validation_steps160 = test_generator160.n // batch_size\n",
    "\n",
    "model9.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model9.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analysis\n",
    "\n",
    "improved performance without Batch Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adding more dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = 160\n",
    "batch_size = 32\n",
    "\n",
    "train_generator160 = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "test_generator160 = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = tf.keras.Sequential()\n",
    "\n",
    "model10.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model10.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model10.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model10.add(layers.MaxPooling2D((2, 2)))\n",
    "model10.add(layers.Dropout(.2))\n",
    "\n",
    "model10.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model10.add(layers.MaxPooling2D((2, 2)))\n",
    "model10.add(layers.Dropout(.2))\n",
    "\n",
    "model10.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "model10.add(layers.MaxPooling2D((2, 2)))\n",
    "model10.add(layers.Dropout(.2))\n",
    "\n",
    "model10.add(layers.Flatten())\n",
    "model10.add(layers.Dense(256, activation='relu'))\n",
    "model10.add(layers.Dropout(.3))\n",
    "model10.add(layers.Dense(128, activation='relu'))\n",
    "model10.add(layers.Dropout(.2))\n",
    "model10.add(layers.Dense(64, activation='relu'))\n",
    "model10.add(layers.Dropout(.1))\n",
    "\n",
    "model10.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "163/163 [==============================] - 288s 2s/step - loss: 0.5253 - acc: 0.7584 - val_loss: 0.4900 - val_acc: 0.8421\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 229s 1s/step - loss: 0.2897 - acc: 0.8806 - val_loss: 0.3709 - val_acc: 0.8553\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 227s 1s/step - loss: 0.2027 - acc: 0.9179 - val_loss: 0.5105 - val_acc: 0.7763\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 233s 1s/step - loss: 0.1551 - acc: 0.9406 - val_loss: 0.3815 - val_acc: 0.8372\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 225s 1s/step - loss: 0.1346 - acc: 0.9469 - val_loss: 0.4697 - val_acc: 0.7829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x140c10780>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch160 = train_generator160.n // batch_size\n",
    "validation_steps160 = test_generator160.n // batch_size\n",
    "\n",
    "model10.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model10.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analysis\n",
    "\n",
    "still overfitting to training data, increasing dropout rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = 160\n",
    "batch_size = 32\n",
    "\n",
    "train_generator160 = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "test_generator160 = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0414 16:09:28.746411 4722408896 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0414 16:09:28.870649 4722408896 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "model11 = tf.keras.Sequential()\n",
    "\n",
    "model11.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model11.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model11.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model11.add(layers.MaxPooling2D((2, 2)))\n",
    "model11.add(layers.Dropout(.3))\n",
    "\n",
    "model11.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model11.add(layers.MaxPooling2D((2, 2)))\n",
    "model11.add(layers.Dropout(.4))\n",
    "\n",
    "model11.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "model11.add(layers.MaxPooling2D((2, 2)))\n",
    "model11.add(layers.Dropout(.6))\n",
    "\n",
    "model11.add(layers.Flatten())\n",
    "model11.add(layers.Dense(256, activation='relu'))\n",
    "model11.add(layers.Dropout(.6))\n",
    "model11.add(layers.Dense(128, activation='relu'))\n",
    "model11.add(layers.Dropout(.4))\n",
    "model11.add(layers.Dense(64, activation='relu'))\n",
    "model11.add(layers.Dropout(.2))\n",
    "model11.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "163/163 [==============================] - 234s 1s/step - loss: 0.6007 - acc: 0.7239 - val_loss: 0.6669 - val_acc: 0.6266\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 219s 1s/step - loss: 0.5311 - acc: 0.7429 - val_loss: 0.5642 - val_acc: 0.6266\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 218s 1s/step - loss: 0.3621 - acc: 0.8129 - val_loss: 0.4023 - val_acc: 0.8602\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 220s 1s/step - loss: 0.2452 - acc: 0.9028 - val_loss: 0.4912 - val_acc: 0.8174\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 232s 1s/step - loss: 0.2082 - acc: 0.9183 - val_loss: 0.3527 - val_acc: 0.8520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1419754a8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch160 = train_generator160.n // batch_size\n",
    "validation_steps160 = test_generator160.n // batch_size\n",
    "\n",
    "model11.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model11.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
